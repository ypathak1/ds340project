"""
ALL 4 MODELS RUN SIMULTANEOUSLY - NO IF STATEMENTS!

STACKING:
1. FER Pretrained ‚Üí Emotion prediction
2. Custom Model ‚Üí Emotion prediction (YOUR face)
3. MediaPipe Face ‚Üí Emotion from landmarks
4. MediaPipe Hands ‚Üí Finger counting

ALL models vote together for maximum accuracy
"""

import cv2
import numpy as np
import mediapipe as mp
from keras.models import load_model, Model
from keras.layers import Input, Dense, Dropout, BatchNormalization
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
import os
from collections import deque, Counter
import threading

try:
    # Older releases export FER at the package root
    from fer import FER
except ImportError:
    try:
        # Newer fer>=25 keeps FER in fer.fer
        from fer.fer import FER  # type: ignore
    except Exception:
        FER = None  # Degrade gracefully if the package is missing

print("=" * 70)
print("GESTURE CALCULATOR - COMPLETE STACKING EDITION")
print("=" * 70)
print("\nüöÄ ALL 4 MODELS STACKING TOGETHER:")
print("  1. FER Pretrained (general emotions)")
print("  2. Custom Model (YOUR face)")
print("  3. MediaPipe Face (landmark emotions)")
print("  4. MediaPipe Hands (finger counting)")
print("\n‚ú® ENSEMBLE VOTING: All predictions combined!")
print("‚ú® WORKS IMMEDIATELY: No training needed!")
print("‚ú® LIVE TRAINING: Press H/S/N/A to add samples!")
print()

# LOAD ALL MODELS
print("[INFO] Loading ALL models (no if statements)...")

# MODEL 1: FER (downloads automatically if needed)
fer_detector = None
try:
    if FER is not None:
        print("  ‚è≥ Loading FER...")
        fer_detector = FER(mtcnn=False)
        print("  ‚úì FER Pretrained active")
    else:
        print("  ‚ö† FER unavailable (pip install fer)")
except:
    print("  ‚ö† FER unavailable (pip install fer)")

# MODEL 2: Custom (loads if exists, creates placeholder if not)
custom_model = None
custom_labels = np.array(['happy', 'sad', 'neutral', 'angry'])

if os.path.exists("custom_emotion_model.h5"):
    try:
        custom_model = load_model("custom_emotion_model.h5")
        custom_labels = np.load("custom_emotion_labels.npy")
        print("  ‚úì Custom Model active")
    except:
        print("  ‚ö† Custom model file corrupted")
else:
    print("  ‚óã Custom Model not trained (will train when you add samples)")

# MODEL 3 & 4: MediaPipe
mp_face_mesh = mp.solutions.face_mesh
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
)

hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
)

print("  ‚úì MediaPipe Face active")
print("  ‚úì MediaPipe Hands active")

# MAPPINGS
EMOTION_TO_OP = {
    'happy': ('*', lambda a, b: a * b, (0, 255, 0)),
    'sad': ('/', lambda a, b: a / b if b != 0 else 0, (255, 100, 100)),
    'neutral': ('+', lambda a, b: a + b, (255, 255, 255)),
    'angry': ('-', lambda a, b: a - b, (0, 100, 255))
}

FER_MAPPING = {
    'happy': 'happy', 'sad': 'sad', 'neutral': 'neutral', 'angry': 'angry',
    'surprise': 'happy', 'fear': 'sad', 'disgust': 'angry'
}

# MODEL 3: MediaPipe Landmark Emotion
def mediapipe_emotion_from_landmarks(face_landmarks):
    """
    Detect emotion using MediaPipe face landmarks
    Uses mouth and eyebrow positions
    """
    try:
        # Get key landmarks
        left_mouth = face_landmarks[61]
        right_mouth = face_landmarks[291]
        upper_lip = face_landmarks[13]
        lower_lip = face_landmarks[14]
        
        left_brow = face_landmarks[70]
        right_brow = face_landmarks[300]
        nose = face_landmarks[168]
        
        # Calculate positions
        mouth_avg_y = (left_mouth.y + right_mouth.y) / 2
        lip_center_y = (upper_lip.y + lower_lip.y) / 2
        brow_avg_y = (left_brow.y + right_brow.y) / 2
        brow_distance = nose.y - brow_avg_y
        
        # Emotion rules
        if mouth_avg_y < lip_center_y - 0.015:  # Smile
            return 'happy'
        elif mouth_avg_y > lip_center_y + 0.01:  # Frown
            return 'sad'
        elif brow_distance < 0.06:  # Brows lowered
            return 'angry'
        else:
            return 'neutral'
    except:
        return 'neutral'

# TRAINING
training_data = {'happy': [], 'sad': [], 'neutral': [], 'angry': []}
is_training = False

def train_model_background():
    global custom_model, is_training
    
    print("\n[TRAIN] Background training...")
    
    all_X = []
    all_y = []
    
    # Load existing
    for emotion in ['happy', 'sad', 'neutral', 'angry']:
        if os.path.exists(f"emotion_{emotion}.npy"):
            data = np.load(f"emotion_{emotion}.npy")
            all_X.append(data)
            all_y.extend([emotion] * len(data))
    
    # Add live data
    for emotion, samples in training_data.items():
        if samples:
            all_X.append(np.array(samples))
            all_y.extend([emotion] * len(samples))
    
    if not all_X:
        print("[TRAIN] No data")
        is_training = False
        return
    
    X = np.vstack(all_X)
    label_map = {'happy': 0, 'sad': 1, 'neutral': 2, 'angry': 3}
    y = to_categorical([label_map[label] for label in all_y], 4)
    
    idx = np.random.permutation(len(X))
    X, y = X[idx], y[idx]
    
    ip = Input(shape=(X.shape[1],))
    x = Dense(256, activation="relu")(ip)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    x = Dense(128, activation="relu")(x)
    x = Dropout(0.2)(x)
    op = Dense(4, activation="softmax")(x)
    
    model = Model(inputs=ip, outputs=op)
    model.compile(optimizer='adam', loss="categorical_crossentropy", metrics=['accuracy'])
    model.fit(X, y, epochs=15, batch_size=16, validation_split=0.15,
              callbacks=[EarlyStopping(patience=3)], verbose=0)
    
    model.save("custom_emotion_model.h5")
    custom_model = model
    
    print("[TRAIN] ‚úì Done!")
    is_training = False

# FINGER COUNTING
def count_fingers(hand_lm, hand_side):
    tips = [4, 8, 12, 16, 20]
    pips = [2, 6, 10, 14, 18]
    count = 0
    
    is_right = hand_side.classification[0].label == "Right"
    
    # Thumb
    if (is_right and hand_lm.landmark[tips[0]].x < hand_lm.landmark[pips[0]].x) or \
       (not is_right and hand_lm.landmark[tips[0]].x > hand_lm.landmark[pips[0]].x):
        count += 1
    
    # Fingers
    for i in range(1, 5):
        if hand_lm.landmark[tips[i]].y < hand_lm.landmark[pips[i]].y:
            count += 1
    
    return count

# ENSEMBLE: ALL MODELS VOTE
def get_ensemble_emotion(frame, face_landmarks_list):
    """
    ALL 4 MODELS PREDICT SIMULTANEOUSLY
    Returns: final_emotion, individual_predictions_dict
    """
    votes = []
    predictions = {}
    
    # Vote 1: FER
    if fer_detector:
        try:
            result = fer_detector.detect_emotions(frame)
            if result:
                emo_dict = result[0]['emotions']
                fer_emo = max(emo_dict, key=emo_dict.get)
                mapped = FER_MAPPING.get(fer_emo, 'neutral')
                predictions['FER'] = mapped
                votes.append(mapped)
        except:
            pass
    
    # Vote 2: Custom
    if custom_model and face_landmarks_list:
        try:
            feats = []
            for lm in face_landmarks_list:
                feats.extend([lm.x, lm.y, lm.z])
            feats = np.array(feats).reshape(1, -1)
            
            probs = custom_model.predict(feats, verbose=0)[0]
            custom_emo = custom_labels[np.argmax(probs)]
            predictions['Custom'] = custom_emo
            votes.append(custom_emo)
        except:
            pass
    
    # Vote 3: MediaPipe Landmarks
    if face_landmarks_list:
        try:
            mp_emo = mediapipe_emotion_from_landmarks(face_landmarks_list)
            predictions['MediaPipe'] = mp_emo
            votes.append(mp_emo)
        except:
            pass
    
    # ENSEMBLE: Majority vote
    if votes:
        final = Counter(votes).most_common(1)[0][0]
        return final, predictions
    
    return 'neutral', predictions

# MAIN
print("[INFO] Starting...")
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print("[ERROR] Webcam failed!")
    exit(1)

print("\n" + "=" * 70)
print("[SUCCESS] ALL 4 MODELS ACTIVE - STACKING ENGAGED!")
print("=" * 70)
print("\nüéÆ CONTROLS:")
print("  Q=Quit | H=Happy S=Sad N=Neutral A=Angry | T=Train")
print("\nüí° Works NOW! Add samples to improve accuracy.\n")

cv2.namedWindow('Gesture Calculator', cv2.WINDOW_NORMAL)
cv2.resizeWindow('Gesture Calculator', 1400, 800)

# State
cur_left = None
cur_right = None
cur_op = '+'
cur_emo = 'neutral'
op_color = (255, 255, 255)
emo_hist = deque(maxlen=7)
left_hist = deque(maxlen=3)
right_hist = deque(maxlen=3)
last_face = None
preds = {}

while True:
    ret, frm = cap.read()
    if not ret:
        break
    
    frm = cv2.flip(frm, 1)
    rgb = cv2.cvtColor(frm, cv2.COLOR_BGR2RGB)
    h, w, c = frm.shape
    
    # Process
    face_res = face_mesh.process(rgb)
    hand_res = hands.process(rgb)
    
    # EMOTION: ALL MODELS
    if face_res and face_res.multi_face_landmarks:
        for face_lm in face_res.multi_face_landmarks:
            last_face = face_lm.landmark
            
            # Get ALL predictions
            ens_emo, preds = get_ensemble_emotion(frm, face_lm.landmark)
            
            emo_hist.append(ens_emo)
            
            if len(emo_hist) >= 4:
                cur_emo = Counter(emo_hist).most_common(1)[0][0]
            
            if cur_emo in EMOTION_TO_OP:
                cur_op, _, op_color = EMOTION_TO_OP[cur_emo][:3]
    
    # HANDS: MediaPipe
    if hand_res.multi_hand_landmarks and hand_res.multi_handedness:
        temp_l = None
        temp_r = None
        
        for hand_lm, hand_side in zip(hand_res.multi_hand_landmarks, hand_res.multi_handedness):
            mp_drawing.draw_landmarks(
                frm, hand_lm, mp_hands.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style()
            )
            
            cnt = count_fingers(hand_lm, hand_side)
            label = hand_side.classification[0].label
            
            if label == "Left":
                temp_l = cnt
            else:
                temp_r = cnt
            
            cx = int(hand_lm.landmark[9].x * w)
            cy = int(hand_lm.landmark[9].y * h)
            cv2.putText(frm, str(cnt), (cx-30, cy-30),
                        cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0,255,255), 5)
        
        if temp_l is not None:
            left_hist.append(temp_l)
            if len(left_hist) >= 2:
                cur_left = int(np.median(list(left_hist)))
        
        if temp_r is not None:
            right_hist.append(temp_r)
            if len(right_hist) >= 2:
                cur_right = int(np.median(list(right_hist)))
    
    # CALCULATE
    result = "?"
    if cur_left is not None and cur_right is not None and cur_emo in EMOTION_TO_OP:
        try:
            _, func, _ = EMOTION_TO_OP[cur_emo]
            res = func(cur_left, cur_right)
            result = f"{res:.2f}" if isinstance(res, float) else str(res)
        except:
            result = "Error"
    
    # UI
    overlay = frm.copy()
    cv2.rectangle(overlay, (0,0), (w,320), (0,0,0), -1)
    cv2.addWeighted(overlay, 0.5, frm, 0.5, 0, frm)
    
    # Equation
    l_str = str(cur_left) if cur_left else "?"
    r_str = str(cur_right) if cur_right else "?"
    eq = f"{l_str} {cur_op} {r_str} = {result}"
    
    cv2.putText(frm, eq, (50,120), cv2.FONT_HERSHEY_SIMPLEX, 3, op_color, 8)
    
    # Emotion + votes
    cv2.putText(frm, f"Emotion: {cur_emo.upper()}", (50,200),
                cv2.FONT_HERSHEY_SIMPLEX, 1.1, (255,255,255), 2)
    
    y = 240
    for model, vote in preds.items():
        color = (0,255,0) if vote == cur_emo else (150,150,150)
        cv2.putText(frm, f"{model}: {vote}", (50,y),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        y += 25
    
    # Training
    if is_training:
        cv2.putText(frm, "TRAINING...", (w-250,50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,255), 3)
    
    total = sum(len(s) for s in training_data.values())
    if total:
        cv2.putText(frm, f"Samples: {total}", (w-250,100),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,0), 2)
    
    # Controls
    cv2.putText(frm, "Q=Quit | H/S/N/A=Add Sample | T=Train", (30,h-70),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
    cv2.putText(frm, "All 4 models voting together!", (30,h-40),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)
    
    cv2.imshow('Gesture Calculator', frm)
    
    # KEYS
    key = cv2.waitKey(1) & 0xFF
    
    if key == ord('q'):
        break
    
    # Live training
    if last_face:
        emo_key = None
        if key == ord('h'):
            emo_key = 'happy'
        elif key == ord('s'):
            emo_key = 'sad'
        elif key == ord('n'):
            emo_key = 'neutral'
        elif key == ord('a'):
            emo_key = 'angry'
        
        if emo_key:
            feats = []
            for lm in last_face:
                feats.extend([lm.x, lm.y, lm.z])
            training_data[emo_key].append(feats)
            print(f"[LIVE] {emo_key}! Total: {len(training_data[emo_key])}")
    
    # Train
    if key == ord('t') and not is_training:
        total = sum(len(s) for s in training_data.values())
        if total >= 10:
            is_training = True
            threading.Thread(target=train_model_background, daemon=True).start()
        else:
            print(f"[TRAIN] Need 10+ samples (have {total})")

cv2.destroyAllWindows()
cap.release()
face_mesh.close()
hands.close()
print("\n[OK] Gesture calculator closed! ‚ú®\n")
